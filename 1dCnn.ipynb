{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "R5NZErQq3-LN",
    "outputId": "4caf8c60-0b2e-4cd2-e856-7a9adaa03ee9"
   },
   "outputs": [],
   "source": [
    "import hgtk\n",
    "import numpy as np\n",
    "\n",
    "import nengo\n",
    "import tensorflow as tf\n",
    "\n",
    "import nengo_dl\n",
    "\n",
    "train_data = open('ratings_train_small.txt', 'r', encoding='utf-8')\n",
    "test_data = open('ratings_test_small.txt', 'r', encoding='utf-8')\n",
    "\n",
    "train_lines = train_data.readlines()\n",
    "test_lines = test_data.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aUtVv59x3-Ld",
    "outputId": "7a3d2a94-741e-49a9-fe8a-cae3dd491c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9976970\\t아 더빙.. 진짜 짜증나네요 목소리\\t0\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_lines[0]\n",
    "del test_lines[0]\n",
    "train_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "EEITKEPY3-Lp",
    "outputId": "01265010-2156-4344-9113-42ef84aea422",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ')\n",
      "('', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ')\n",
      "('ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ')\n",
      "total_cho len: 19\n",
      "total_joong len: 21\n",
      "total_jong len: 28\n",
      "total sent len = 68\n"
     ]
    }
   ],
   "source": [
    "total_cho = hgtk.josa.CHO\n",
    "total_joong = hgtk.josa.JOONG\n",
    "total_jong = hgtk.josa.JONG\n",
    "print(total_cho)\n",
    "print(total_jong)\n",
    "print(total_joong)\n",
    "print('total_cho len: ' + str(len(total_cho)))\n",
    "print('total_joong len: ' + str(len(total_joong)))\n",
    "print('total_jong len: ' + str(len(total_jong)))\n",
    "print('total sent len = ' + str(len(total_cho) + len(total_joong) + len(total_jong)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QBjUbXc73-L0",
    "outputId": "8888d18a-fa92-404d-a77e-5df0a6a3f64c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4624\n"
     ]
    }
   ],
   "source": [
    "max_sent_size = 68 * 68\n",
    "print(max_sent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GAPuZ59t3-L9"
   },
   "outputs": [],
   "source": [
    "def get_sent_vec(ori_sent):\n",
    "    sub_num = 68 - len(ori_sent)\n",
    "    if sub_num > 0:\n",
    "        for num in range(0, sub_num):\n",
    "            ori_sent += \" \"\n",
    "    if len(ori_sent) > 68:\n",
    "        ori_sent = ori_sent[0:68]\n",
    "    jamo_sent = hgtk.text.decompose(ori_sent)\n",
    "    jamo_sent = jamo_sent.replace('ᴥ', '')\n",
    "    #print(jamo_sent + '\\n')\n",
    "\n",
    "    many_hot = np.zeros(len(total_cho + total_joong + total_jong))\n",
    "    sent_hot = np.zeros((len(ori_sent),len(many_hot)))\n",
    "    word_idx = 0\n",
    "\n",
    "    for word in ori_sent:\n",
    "        #print(\"input_word: \" + str(word))\n",
    "        if(hgtk.checker.is_hangul(word)):\n",
    "            split_word = hgtk.letter.decompose(word)\n",
    "            cho_idx = 0\n",
    "            joong_idx = 0\n",
    "            jong_idx = 0\n",
    "            one_hot_cho = np.zeros(len(hgtk.josa.CHO))\n",
    "            one_hot_joong = np.zeros(len(hgtk.josa.JOONG))\n",
    "            one_hot_jong = np.zeros(len(hgtk.josa.JONG))\n",
    "            if(len(split_word) > 2):\n",
    "                for cho in total_cho:\n",
    "                    if(cho == split_word[0]):\n",
    "                        one_hot_cho[cho_idx] = 1\n",
    "                        break\n",
    "                    else:\n",
    "                        cho_idx+= 1\n",
    "                for joong in total_joong:\n",
    "                    if(joong == split_word[1]):\n",
    "                        one_hot_joong[joong_idx] = 1\n",
    "                        break\n",
    "                    else:\n",
    "                        joong_idx+= 1\n",
    "                for jong in total_jong:\n",
    "                    if(jong == split_word[2]):\n",
    "                        one_hot_jong[jong_idx] = 1\n",
    "                        break\n",
    "                    else:\n",
    "                        jong_idx+= 1\n",
    "            else:\n",
    "                for cho in total_cho:\n",
    "                    if(cho == split_word[0]):\n",
    "                        one_hot_cho[cho_idx] = 1\n",
    "                        break\n",
    "                    else:\n",
    "                        cho_idx+= 1\n",
    "                for joong in total_joong:\n",
    "                    if(joong == split_word[1]):\n",
    "                        one_hot_joong[joong_idx] = 1\n",
    "                        break\n",
    "                    else:\n",
    "                        joong_idx+= 1\n",
    "            many_hot = np.append(one_hot_cho, one_hot_joong)\n",
    "            many_hot = np.append(many_hot, one_hot_jong)\n",
    "\n",
    "        else:\n",
    "            many_hot = np.zeros(len(total_cho + total_joong + total_jong))\n",
    "        sent_hot[word_idx] = many_hot\n",
    "        word_idx += 1\n",
    "    return sent_hot.reshape(1, max_sent_size)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "VNDo3HVe3-MM",
    "outputId": "a5a4737a-5d58-438c-faf1-33c5145c6688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "4624\n"
     ]
    }
   ],
   "source": [
    "sent_hot = get_sent_vec(train_lines[1].split('\\t')[1])\n",
    "print(sent_hot)\n",
    "print(len(sent_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tPpgh_kp3-MT"
   },
   "outputs": [],
   "source": [
    "train_data_array = []\n",
    "sent_vec_array = []\n",
    "label_array = []\n",
    "for line in train_lines:\n",
    "    sent = line.split('\\t')[1]\n",
    "    label = line.split('\\t')[2].replace('\\n', '')\n",
    "    sent_vec = get_sent_vec(sent)\n",
    "    sent_vec_array.append(list(sent_vec))\n",
    "    label_array.append(int(label))\n",
    "\n",
    "train_data_array = [np.array(sent_vec_array), np.array(label_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]), array([1, 0, 0, ..., 0, 1, 0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1ZaApbI9F9p"
   },
   "outputs": [],
   "source": [
    "test_data_array = []\n",
    "sent_vec_array = []\n",
    "label_array = []\n",
    "for line in test_lines:\n",
    "    sent = line.split('\\t')[1]\n",
    "    label = line.split('\\t')[2].replace('\\n', '')\n",
    "    sent_vec = get_sent_vec(sent)\n",
    "    sent_vec_array.append(list(sent_vec))\n",
    "    label_array.append(int(label))\n",
    "\n",
    "test_data_array = [np.array(sent_vec_array), np.array(label_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4624"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_array[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(68*68) / 68*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "roF0Mfty3-Mp",
    "outputId": "f771313a-db41-4761-803f-05709d9dc37a"
   },
   "outputs": [],
   "source": [
    "with nengo.Network() as net:\n",
    "    # set some default parameters for the neurons that will make\n",
    "    # the training progress more smoothly\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([100])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    neuron_type = nengo.LIF(amplitude=0.01)\n",
    "    #neuron_type = nengo.SpikingRectifiedLinear()\n",
    "    \n",
    "    # we'll make all the nengo objects in the network\n",
    "    # non-trainable. we could train them if we wanted, but they don't\n",
    "    # add any representational power. note that this doesn't affect \n",
    "    # the internal components of tensornodes, which will always be \n",
    "    # trainable or non-trainable depending on the code written in \n",
    "    # the tensornode.\n",
    "    nengo_dl.configure_settings(trainable=False)\n",
    "\n",
    "    # the input node that will be used to feed in input images \n",
    "    inp = nengo.Node([0] * 4624)\n",
    "\n",
    "    # add the first convolutional layer\n",
    "    x = nengo_dl.tensor_layer(\n",
    "        inp, tf.layers.conv1d, shape_in=(68*68, 1), filters=1, kernel_size=68*3)\n",
    "\n",
    "    # apply the neural nonlinearity\n",
    "    x = nengo_dl.tensor_layer(x, neuron_type)\n",
    "# filters랑 shape_in의 마지막 숫자랑 맞춰줘야해 \n",
    "# kernel size만큼 돌면 남는게 output임\n",
    "    # add another convolutional layer\n",
    "    x = nengo_dl.tensor_layer(\n",
    "        x, tf.layers.conv1d, shape_in=(68*68-(68*3-1), 1), filters=1, kernel_size=68*2)\n",
    "    x = nengo_dl.tensor_layer(x, neuron_type)\n",
    "\n",
    "    # add a pooling layer\n",
    "    x = nengo_dl.tensor_layer(\n",
    "        x, tf.layers.average_pooling1d, shape_in=(68*68-(68*3-1)-(68*2-1), 1), pool_size=2, strides=2)\n",
    "\n",
    "    # another convolutional layer\n",
    "    x = nengo_dl.tensor_layer(\n",
    "        x, tf.layers.conv1d, shape_in=(2143, 1), filters=1, kernel_size=68*2)\n",
    "    x = nengo_dl.tensor_layer(x, neuron_type)\n",
    "\n",
    "    # another pooling layer\n",
    "    x = nengo_dl.tensor_layer(\n",
    "        x, tf.layers.average_pooling1d, shape_in=(2143-(68*2-1), 1), pool_size=2, strides=2)\n",
    "\n",
    "\n",
    "    # linear readout\n",
    "    x = nengo_dl.tensor_layer(x, tf.layers.dense, units=2)\n",
    "\n",
    "    # we'll create two different output probes, one with a filter\n",
    "    # (for when we're simulating the network over time and\n",
    "    # accumulating spikes), and one without (for when we're\n",
    "    # training the network using a rate-based approximation)\n",
    "    out_p = nengo.Probe(x)\n",
    "    out_p_filt = nengo.Probe(x, synapse=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "fVtKhHxD3-Mz",
    "outputId": "cee5ad8a-1048-4d15-e408-84e17926a191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:01                                               ##################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################| ETA: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "minibatch_size = 16\n",
    "sim = nengo_dl.Simulator(net, minibatch_size=minibatch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiqeBmAeBrSo"
   },
   "outputs": [],
   "source": [
    "data = train_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "Gc15htsEDLDk",
    "outputId": "78e6f069-cc4f-4a58-a63b-a95602a9aed7"
   },
   "outputs": [],
   "source": [
    "for data in (train_data_array, test_data_array):\n",
    "    one_hot = np.zeros((data[0].shape[0], 2))\n",
    "    one_hot[np.arange(data[0].shape[0]), data[1]] = 1\n",
    "    data[1] = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4vCdzpP3-M7"
   },
   "outputs": [],
   "source": [
    "# add the single timestep to the training data\n",
    "train_data_array = {inp: train_data_array[0][:, None, :],\n",
    "              out_p: train_data_array[1][:, None, :]}\n",
    "\n",
    "# when testing our network with spiking neurons we will need to run it \n",
    "# over time, so we repeat the input/target data for a number of \n",
    "# timesteps. we're also going to reduce the number of test images, just \n",
    "# to speed up this example.\n",
    "n_steps = 30\n",
    "test_data_array = {\n",
    "    inp: np.tile(test_data_array[0][:minibatch_size*2, None, :],\n",
    "                 (1, n_steps, 1)),\n",
    "    out_p_filt: np.tile(test_data_array[1][:minibatch_size*2, None, :],\n",
    "                        (1, n_steps, 1))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJ-lYcsSDalv"
   },
   "outputs": [],
   "source": [
    "def objective(outputs, targets): \n",
    "    return tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=outputs, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sG9Nj_X0DfzK"
   },
   "outputs": [],
   "source": [
    "opt = tf.train.RMSPropOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "2YPqyewWDhCh",
    "outputId": "9ee9c4a5-7d93-44e2-c7a7-8fa0b040cdfd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation finished in 0:00:01                                                \n",
      "error before training: 53.12%\n"
     ]
    }
   ],
   "source": [
    "def classification_error(outputs, targets):\n",
    "    return 100 * tf.reduce_mean(\n",
    "        tf.cast(tf.not_equal(tf.argmax(outputs[:, -1], axis=-1), \n",
    "                             tf.argmax(targets[:, -1], axis=-1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "print(\"error before training: %.2f%%\" % sim.loss(\n",
    "    test_data_array, {out_p_filt: classification_error}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsNgrAs2DiBP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                   Training (0%)                  | ETA:  --:--:-- (loss: ---)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banana/.local/lib/python3.5/site-packages/nengo_dl/utils.py:489: UserWarning: Number of data elements (9999) is not an even multiple of minibatch size (16); inputs will be truncated\n",
      "  (n_inputs, minibatch_size)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 0:01:57 (loss: 11.0789)                                   \n"
     ]
    }
   ],
   "source": [
    "do_training = True\n",
    "if do_training:\n",
    "    # run training\n",
    "    sim.train(train_data_array, opt, objective={out_p: objective}, n_epochs=10)\n",
    "    \n",
    "    # save the parameters to file\n",
    "    sim.save_params(\"model_output/nsmc_params\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation finished in 0:00:00                                                \n",
      "error after training: 53.12%\n"
     ]
    }
   ],
   "source": [
    "print(\"error after training: %.2f%%\" % sim.loss(\n",
    "    test_data_array, {out_p_filt: classification_error}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "make_train_data.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
